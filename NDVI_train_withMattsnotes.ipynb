{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6b21f-1379-4f68-ba44-c979036e539d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Common imports and settings\n",
    "import os, sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "import xarray as xr\n",
    "\n",
    "# Datacube\n",
    "import datacube\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils import masking\n",
    "from datacube.utils.cog import write_cog\n",
    "# https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop/Tools\n",
    "from dea_tools.plotting import display_map, rgb\n",
    "from dea_tools.datahandling import mostcommon_crs\n",
    "\n",
    "# EASI defaults\n",
    "easinotebooksrepo = '/home/jovyan/easi-notebooks'\n",
    "if easinotebooksrepo not in sys.path: sys.path.append(easinotebooksrepo)\n",
    "from easi_tools import EasiDefaults, xarray_object_size, notebook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc7f71-3aaa-4f48-9b06-c990bac3c392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Datacube\n",
    "from datacube.utils import masking  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/masking.py\n",
    "from odc.algo import enum_to_bool   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_masking.py\n",
    "from odc.algo import xr_reproject   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_warp.py\n",
    "from datacube.utils.geometry import GeoBox, box  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/geometry/_base.py\n",
    "\n",
    "# Holoviews, Datashader and Bokeh\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import colorcet as cc\n",
    "import cartopy.crs as ccrs\n",
    "from datashader import reductions\n",
    "from holoviews import opts\n",
    "# import geoviews as gv\n",
    "# from holoviews.operation.datashader import rasterize\n",
    "hv.extension('bokeh', logo=False)\n",
    "\n",
    "# Dask\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5cc87-77a7-4734-ae3e-13d7360624c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "easi = EasiDefaults()\n",
    "\n",
    "family = 'sentinel-2'\n",
    "product = easi.product(family)\n",
    "display(Markdown(f'Default {family} product for \"{easi.name}\": [{product}]({easi.explorer}/products/{product})'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72901e1e-2af3-408c-816c-4c3412575de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "# Default is to run on a compute node with 28 GiB of available memory and 8 cores.\n",
    "# We'll make that explicit here .. but this should be adjusted based on your workflow\n",
    "\n",
    "# We can try different combinations of number of workers and memory per worker\n",
    "# Maybe try:\n",
    "# cluster.scale(n=4, memory=\"6GiB\")\n",
    "# cluster.scale(n=8, memory=\"3GiB\")\n",
    "# We can also try using a \"dask-gateway\" cluster > spin up many new worker pods with their own cpu/memory\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=4)\n",
    "cluster.scale(n=2, memory=\"14GiB\")\n",
    "client = Client(cluster)\n",
    "display(client)\n",
    "\n",
    "dashboard_address = notebook_utils.localcluster_dashboard(client=client,server=easi.hub)\n",
    "display(dashboard_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45e34-f8f3-4e38-b973-cd75a8dabc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube()\n",
    "\n",
    "# Access AWS \"requester-pays\" buckets\n",
    "# This is necessary for reading data from most third-party AWS S3 buckets such as for Landsat and Sentinel-2\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614ee7b-1f84-4f65-ba6b-09ecf790fd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_data_geo\n",
    "import geopandas as gpd\n",
    "from deafrica_tools.areaofinterest import define_area\n",
    "from datacube.utils.geometry import Geometry\n",
    "import xarray as xr\n",
    "train_path = \"train/Soc Trang_Traning.shp\"\n",
    "train = load_data_geo(train_path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e555881-6bed-4c0b-8b81-15ef4e7aef10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29617c37-879d-4787-abfa-64a84b01d0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f362c-ea1f-47be-92ee-7d5083fdaa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head().explore(column=\"Name\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de4800-48e7-4185-816c-1015db2039a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bdb6f-6605-4ee0-9879-0cdc49e897a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_date = '2022-01-01' # 2021-11-01\n",
    "max_date = '2022-02-01' # 2022-01-01\n",
    "product = 's2_l2a'\n",
    "\n",
    "loaded_datasets = {}\n",
    "\n",
    "\n",
    "# Current workflow (I think)\n",
    "# for each training point\n",
    "# - get S2 data (utm)\n",
    "# - apply mask\n",
    "# - for each red,green,blue,nir\n",
    "#    - apply scale/offset\n",
    "#    - persist\n",
    "#    - stack (result.merge)\n",
    "# - add (dask xarray) to loaded_datasets dict\n",
    "# - for each dict item\n",
    "#    - calculate NDVI for the point's dask xarray\n",
    "#    - do the actual read data and calculations\n",
    "#    - = value at that point\n",
    "\n",
    "\n",
    "# Proposed workflow\n",
    "# 1. get bounding polygon for all training data points\n",
    "# 2. dc.load with dask for bounding polygon (and all times when you're ready to try that)\n",
    "#     - consider also remapping S2 data to lat/lon projection (e.g., epsg:4326) - may not be necessary\n",
    "# 2a. apply S2 masking, scale, offset\n",
    "# 3. calculate NDVI (still in dask so its a \"virtual\" on-demand calculation)\n",
    "# 3a. use xarray.persist() to pre-calculate NDVI for all pixels in our bounding polygon\n",
    "#     - more efficient to read and process all pixels than process each training point\n",
    "# 4. for idx, point in train.iterrows():\n",
    "#     -  get points from xarray (dask)\n",
    "#        need to convert point lat/lon to S2 UTM or dc.load into epsg:4326\n",
    "#        xarray data in S2 UTM project (output_crs, resolution)\n",
    "#        point data in epsg:4326 (train.crs)\n",
    "#     -  Store the loaded point data in the dictionary with a key based on the point index\n",
    "\n",
    "# Test or check CRSs\n",
    "# - Change training data (and geopolygon) CRS to \"ncrs\" (most common)\n",
    "# - Or Load dc data into training data CRS (output_crs=epsg:4326, resolution:(-0.0001, 0.0001) (approximate degrees equivalent of 10 m)\n",
    "\n",
    "\n",
    "# Iterate over each point in the GeoDataFrame\n",
    "for idx, point in train.iterrows():\n",
    "    # Create a bounding box around the point\n",
    "    aoi = define_area(lat=point.geometry.y, lon=point.geometry.x, buffer=0)\n",
    "    geopolygon = Geometry(aoi[\"features\"][0][\"geometry\"], crs=train.crs)\n",
    "    geopolygon_gdf = gpd.GeoDataFrame(geometry=[geopolygon], crs=train.crs)\n",
    "    # Get the latitude and longitude range of the geopolygon\n",
    "    lat_range = (geopolygon_gdf.total_bounds[1], geopolygon_gdf.total_bounds[3])\n",
    "    lon_range = (geopolygon_gdf.total_bounds[0], geopolygon_gdf.total_bounds[2])\n",
    "    #print(geopolygon_gdf.total_bounds)\n",
    "    query = {\n",
    "            \"product\": product,\n",
    "            \"x\": lon_range,   # default assumed crs is epsg:4326, which is fine\n",
    "            \"y\": lat_range,\n",
    "            \"time\": (min_date, max_date),\n",
    "    }\n",
    "    ncrs = notebook_utils.mostcommon_crs(dc, query)   # UTM for the \"most common\" S2 MGRS grid\n",
    "    # print(ncrs)\n",
    "    query.update({\n",
    "            \"output_crs\": ncrs,\n",
    "            \"resolution\": (-10, 10),\n",
    "            \"dask_chunks\": {'x': 2048, 'y': 2048}\n",
    "     })\n",
    "    # print(query)\n",
    "    # print(qr)\n",
    "    # break\n",
    "    data = dc.load(**query)  # UTM for the \"most common\" S2 MGRS grid\n",
    "\n",
    "    # Store the loaded dataset in the dictionary with a key based on the point index\n",
    "    key = f'point_{idx + 1}'\n",
    "    \n",
    "    valid_mask = masking.valid_data_mask(data)\n",
    "    \n",
    "    measurement_info = dc.list_measurements().loc[query['product']]\n",
    "\n",
    "    # Separate lists of measurement names and flag names\n",
    "    measurement_names = measurement_info[ pd.isnull(measurement_info.flags_definition)].index\n",
    "    flag_names        = measurement_info[pd.notnull(measurement_info.flags_definition)].index\n",
    "    for flag in flag_names:\n",
    "        notebook_utils.heading(f'Flag definition table for flag name: {flag}')\n",
    "    display(masking.describe_variable_flags(data[flag]))\n",
    "    flags_def = masking.describe_variable_flags(data[flag]).values\n",
    "    flags_def = flags_def.tolist()[0][1]\n",
    "    flag_name = 'scl'\n",
    "    flag_data = data[[flag_name]].where(valid_mask[flag_name]).persist()   # Dataset\n",
    "\n",
    "    good_pixel_flags = [flags_def[str(i)] for i in [4, 5, 6]]\n",
    "\n",
    "    good_pixel_mask = enum_to_bool(data[flag_name], good_pixel_flags)\n",
    "    rs = []\n",
    "    for layer_name in ['red', 'green', 'blue', 'nir']:\n",
    "\n",
    "        # Get scaling and offset values from product description\n",
    "        scale = measurement_info.loc[layer_name].scale_factor\n",
    "        offset = measurement_info.loc[layer_name].add_offset\n",
    "\n",
    "        # Apply valid mask and good pixel mask\n",
    "        layer = data[[layer_name]].where(valid_mask[layer_name] & good_pixel_mask) * scale + offset\n",
    "        layer = layer.persist()\n",
    "        rs.append(layer)\n",
    "    result = rs[0].merge(rs[1])\n",
    "    result = result.merge(rs[2])\n",
    "    result = result.merge(rs[3])\n",
    "    \n",
    "    loaded_datasets[key] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960e503-51a3-4b9a-a871-d45067134e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(loaded_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c464b5-cdda-4105-a7f6-3ed55fab0f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deafrica_tools.bandindices import calculate_indices\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1533ecf-a302-4d9c-88f8-4995d7c2fb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndivi_dataset = {}\n",
    "for i in loaded_datasets.keys():\n",
    "    tmp = calculate_indices(loaded_datasets[i], index='NDVI', satellite_mission='s2')\n",
    "    ndivi_dataset[i] = tmp.NDVI.mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4c5bb1-92bf-4ea6-9fe9-0273b4b348ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndivi_dataset[\"point_1\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb9db0-2c2a-4a73-b377-a429f9f74698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndivi_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf295a9-9628-4acb-9404-1f319075c32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ndivi_dataset['point_1'].plot(cmap='RdYlGn',\n",
    "#            size=6, vmin=-2, vmax=2,\n",
    "# col_wrap=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef59bea-da4d-4b09-9ffd-675b5721318a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndivi_dataset['point_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a5eb5-f893-42f9-8400-587cc3105f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = train.Name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec6db9f-bb9c-4f5e-843e-8d63ac92913f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in ndivi_dataset.keys():\n",
    "    X.append(ndivi_dataset[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa2b45-8dd2-4f40-9c0f-3922a422b3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(arr.shape for arr in X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1ae027-7389-420f-a6e7-9013547353a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfc3d8-8ffe-4d97-b7ea-df59e748475b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "numeric_labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c50ee97-2f63-4753-95f2-192a1abf7e88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(labels, numeric_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9732eb-c4e8-47d6-88bf-9b2f81e31465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_flat = np.vstack([arr.flatten() for arr in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a086f-e828-408e-baa8-b5b5bf44374d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vt_nan = [i[0] for i in np.argwhere(np.isnan(X_flat))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9781a7-2f25-4fdf-894c-73ad8673cb8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_new = []\n",
    "lb_new = []\n",
    "for i in range(len(X)):\n",
    "    if i not in vt_nan:\n",
    "        x_new.append(X[i])\n",
    "        lb_new.append(numeric_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338d607-c969-44e7-98de-8a25291c5bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_new = np.vstack([arr.flatten() for arr in x_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae898c94-e7c9-49a6-8f92-b84fa2c57dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_new, lb_new, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f615c17-cd45-4e1b-a928-50a51f1114c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=150, random_state=42, criterion='gini', max_depth=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b769606a-cb69-4774-9a49-b28839033fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b68a94-6fdf-436c-845b-0948cbc5aa38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f18a86-8796-43cb-a1c4-afa648e7803b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb46ad8-a7e2-4bcf-abba-401de8b18bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vietnam\n",
    "min_longitude, max_longitude = (105.5, 106.4)\n",
    "min_latitude, max_latitude = (9.2, 10.0)\n",
    "min_date = '2022-01-01' # 2021-11-01\n",
    "max_date = '2022-02-01' # 2022-01-01\n",
    "product = 's2_l2a'\n",
    "\n",
    "query1 = {\n",
    "    'product': product,                     # Product name\n",
    "    'x': (min_longitude, max_longitude),    # \"x\" axis bounds\n",
    "    'y': (min_latitude, max_latitude),      # \"y\" axis bounds\n",
    "    'time': (min_date, max_date),           # Any parsable date strings\n",
    "}\n",
    "\n",
    "# Most common CRS\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query1)\n",
    "\n",
    "query1.update({\n",
    "    'output_crs': native_crs,               # EPSG code\n",
    "    'resolution': (-10, 10),                # Target resolution\n",
    "    'group_by': 'solar_day',                # Scene ordering\n",
    "    'dask_chunks': {'x': 2048, 'y': 2048},  # Dask chunks\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1332d-1b59-4102-a702-ef869939863a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = dc.load(**query1)\n",
    "\n",
    "notebook_utils.heading(notebook_utils.xarray_object_size(data))\n",
    "display(data)\n",
    "\n",
    "# Calculate valid (not nodata) masks for each layer\n",
    "valid_mask = masking.valid_data_mask(data)\n",
    "notebook_utils.heading('Valid data masks for each variable')\n",
    "display(valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0dda8-5c36-4c2b-befc-ea5fd54486a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Measurement definitions for the selected product\n",
    "measurement_info = dc.list_measurements().loc[query1['product']]\n",
    "notebook_utils.heading(f'Measurement table for product: {query1[\"product\"]}')\n",
    "display(measurement_info)\n",
    "\n",
    "# Separate lists of measurement names and flag names\n",
    "measurement_names = measurement_info[pd.isnull(measurement_info.flags_definition)].index\n",
    "flag_names        = measurement_info[pd.notnull(measurement_info.flags_definition)].index\n",
    "\n",
    "notebook_utils.heading('Selected Measurement and Flag names')\n",
    "display(pd.DataFrame({\n",
    "    'group': ['Measurement names', 'Flag names'],\n",
    "    'names': [', '.join(measurement_names), ', '.join(flag_names)]\n",
    "}))\n",
    "\n",
    "# Flag definitions\n",
    "for flag in flag_names:\n",
    "    notebook_utils.heading(f'Flag definition table for flag name: {flag}')\n",
    "    display(masking.describe_variable_flags(data[flag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7fc8f-1de7-4e97-9777-b488864bab01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flags_def = masking.describe_variable_flags(data[flag]).values\n",
    "flags_def = flags_def.tolist()[0][1]\n",
    "# Make SCL flags image\n",
    "flag_name = 'scl'\n",
    "flag_data = data[[flag_name]].where(valid_mask[flag_name]).persist()   # Dataset\n",
    "display(flag_data)\n",
    "# Create Mask layer\n",
    "\n",
    "good_pixel_flags = [flags_def[str(i)] for i in [4, 5, 6]]\n",
    "\n",
    "good_pixel_mask = enum_to_bool(data[flag_name], good_pixel_flags)  # -> DataArray\n",
    "# display(good_pixel_mask)  # Type: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb5e57-baea-49e5-8a99-6341ea9cf3bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select a layer and apply masking and scaling, then persist in dask\n",
    "# layer_name = 'red'\n",
    "rs = []\n",
    "for layer_name in ['red', 'green', 'blue', 'nir']:\n",
    "\n",
    "    # Get scaling and offset values from product description\n",
    "    scale = measurement_info.loc[layer_name].scale_factor\n",
    "    offset = measurement_info.loc[layer_name].add_offset\n",
    "\n",
    "    # Apply valid mask and good pixel mask\n",
    "    layer = data[[layer_name]].where(valid_mask[layer_name] & good_pixel_mask) * scale + offset\n",
    "    layer = layer.persist()\n",
    "    rs.append(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437930cc-bb8b-4af8-bd1c-2961168d9be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "result = rs[0].merge(rs[1])\n",
    "result = result.merge(rs[2])\n",
    "result = result.merge(rs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3d656-aeff-40fb-832b-feda881260a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = calculate_indices(result, index='NDVI', satellite_mission='s2')\n",
    "ndvi = ds1[\"NDVI\"]\n",
    "average_ndvi = ndvi.mean(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7923456d-2d40-4bb0-88c6-d2ae882e991a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e079832-333b-4c12-bd82-ad5fed46f84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_array = xr.DataArray(np.zeros((8874, 9902)), dims=('y', 'x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc2e27-d09d-4dc9-b5ee-e1cf2a93bf10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(average_ndvi.values.shape[0]):\n",
    "    for j in range(average_ndvi.values.shape[1]):\n",
    "        x = average_ndvi.values[i][j]\n",
    "        if np.isnan(x):\n",
    "            data_array[i][j] = -1\n",
    "        else:\n",
    "            data_array[i][j] = model.predict([[x]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f706b-1683-400f-b838-faaa3958e790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_ndvi[\"labels\"] = data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81a576-c0e5-43f0-982b-b5f7483be69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42990949-d84e-49fd-9288-ebb6b850bd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
