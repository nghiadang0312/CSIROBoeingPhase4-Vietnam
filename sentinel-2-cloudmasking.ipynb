{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sentinel-2 (global) <img align=\"right\" src=\"../../resources/easi_logo.jpg\">\n",
    "\n",
    "#### Index\n",
    "- [Overview](#Overview)\n",
    "- [Setup (dask, imports, query)](#Setup)\n",
    "- [Product definition (measurements, flags)](#Product-definition)\n",
    "- [Quality layer (mask)](#Quality-layer)\n",
    "- [Scaling and nodata](#Scaling-and-nodata)\n",
    "- [Visualisation](#Visualisation)\n",
    "- [Optional: Reproject](#Optional:-Reproject)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "Sentinel-2 is an Earth observation mission from the EU Copernicus Programme that systematically acquires optical imagery at high spatial resolution (up to 10 m for some bands). The mission is a constellation of two identical satellites in the same orbit, 180Â° apart for optimal coverage and data delivery. Together, they cover all Earth's land surfaces, large islands, inland and coastal waters every 3-5 days.\n",
    "\n",
    "Sentinel-2A was launched on 23 June 2015 and Sentinel-2B followed on 7 March 2017.\n",
    "Both of the Sentinel-2 satellites carry a wide swath high-resolution multispectral imager with 13 spectral bands.\n",
    "For more information on the Sentinel-2 platforms and applications see the [European Space Agency website](http://www.esa.int/Applications/Observing_the_Earth/Copernicus/Overview4).\n",
    "\n",
    "_Adapted from https://github.com/GeoscienceAustralia/dea-notebooks/blob/develop/DEA_datasets/Sentinel_2.ipynb_\n",
    "\n",
    "#### Data source and documentation\n",
    "\n",
    "EASI provides at least four Sentinel-2 products from three sources of data. These represent Analysis Ready Data (ARD) products corrected to surface reflectance.\n",
    "\n",
    "We describe in this notebook the products that can be used in a global context. That is, products processed with globaly applicable algorithms.\n",
    "\n",
    "| Name | Product | Information |\n",
    "|------|---------|-------------|\n",
    "| Sentinel-2 COGs | `s2_l2a` | - Direct indexing from Sentinel-2 Cloud-Optimized GeoTIFFs. [More information below](#Sentinel-2-Cloud-Optimized-GeoTIFFs)<br>- Use for global AOIs where Sen2cor surface correction is required |\n",
    "\n",
    "#### EASI pipeline\n",
    "\n",
    "| Task | `s2_l2a` |\n",
    "|------|------------------------|\n",
    "| Source | https://registry.opendata.aws/sentinel-2-l2a-cogs |\n",
    "| Download | Index in place |\n",
    "| Prepare | stac-to-dc |\n",
    "| TODO | Update \"band_0X\" aliases<br>Capitalise SLC flag #9 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Common imports and settings\n",
    "import os, sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "import xarray as xr\n",
    "\n",
    "# Datacube\n",
    "import datacube\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils import masking\n",
    "from datacube.utils.cog import write_cog\n",
    "# https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop/Tools\n",
    "from dea_tools.plotting import display_map, rgb\n",
    "from dea_tools.datahandling import mostcommon_crs\n",
    "\n",
    "# EASI defaults\n",
    "easinotebooksrepo = '/home/jovyan/easi-notebooks'\n",
    "if easinotebooksrepo not in sys.path: sys.path.append(easinotebooksrepo)\n",
    "from easi_tools import EasiDefaults, xarray_object_size, notebook_utils\n",
    "from easi_tools.s2l2a_patch import load_s2l2a_with_offset_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Datacube\n",
    "from datacube.utils import masking  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/masking.py\n",
    "from odc.algo import enum_to_bool   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_masking.py\n",
    "from odc.algo import xr_reproject   # https://github.com/opendatacube/odc-tools/blob/develop/libs/algo/odc/algo/_warp.py\n",
    "from datacube.utils.geometry import GeoBox, box  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/geometry/_base.py\n",
    "\n",
    "# Holoviews, Datashader and Bokeh\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import colorcet as cc\n",
    "import cartopy.crs as ccrs\n",
    "from datashader import reductions\n",
    "from holoviews import opts\n",
    "# import geoviews as gv\n",
    "# from holoviews.operation.datashader import rasterize\n",
    "hv.extension('bokeh', logo=False)\n",
    "\n",
    "# Dask\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### EASI defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "easi = EasiDefaults()\n",
    "\n",
    "family = 'sentinel-2'\n",
    "product = easi.product(family)\n",
    "display(Markdown(f'Default {family} product for \"{easi.name}\": [{product}]({easi.explorer}/products/{product})'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "# Default is to run on a compute node with 28 GiB of available memory and 8 cores.\n",
    "# We'll make that explicit here .. but this should be adjusted based on your workflow\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=4)\n",
    "cluster.scale(n=2, memory=\"14GiB\")\n",
    "client = Client(cluster)\n",
    "display(client)\n",
    "\n",
    "dashboard_address = notebook_utils.localcluster_dashboard(client=client,server=easi.hub)\n",
    "display(dashboard_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ODC database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube()\n",
    "\n",
    "# Access AWS \"requester-pays\" buckets\n",
    "# This is necessary for reading data from most third-party AWS S3 buckets such as for Landsat and Sentinel-2\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example query\n",
    "\n",
    "Change any of the parameters in the `query` object below to adjust the location, time, projection, or spatial resolution of the returned datasets.\n",
    "\n",
    "Use the Explorer interface to check the temporal and spatial coverage for each product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'{easi.explorer}/products/{product}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vietnam\n",
    "min_longitude, max_longitude = (105.5, 106.4)\n",
    "min_latitude, max_latitude = (9.2, 10.0)\n",
    "min_date = '20y20-05-31'\n",
    "max_date = '2020-06-01'\n",
    "\n",
    "query = {\n",
    "    'product': product,                     # Product name\n",
    "    'x': (min_longitude, max_longitude),    # \"x\" axis bounds\n",
    "    'y': (min_latitude, max_latitude),      # \"y\" axis bounds\n",
    "    'time': (min_date, max_date),           # Any parsable date strings\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Most common CRS\n",
    "\n",
    "Sentinel-2 datasets are stored with different coordinate reference systems (CRS), corresponding to multiple UTM zones used for S2 L1B tiling. S2 measurement bands also have different resolutions (10 m, 20 m and 60 m). As such S2 queries need to include the following two query parameters:\n",
    "\n",
    "* `output_crs` - This sets a consistent CRS that all Sentinel-2 data will be reprojected to, irrespective of the UTM zone the individual image is stored in.\n",
    "* `resolution` - This sets the resolution that all Sentinel-2 images will be resampled to. \n",
    "\n",
    "Use `mostcommon_crs()` to select a CRS. Adapted from https://github.com/GeoscienceAustralia/dea-notebooks/blob/develop/Tools/dea_tools/datahandling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Most common CRS\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query)\n",
    "print(f'Most common native CRS: {native_crs}')\n",
    "\n",
    "# Target xarray params\n",
    "# Usually we group input scenes on the same day to a single time layer\n",
    "# but we need to apply the offset to each input scene\n",
    "# so we will \"group by day\" after the merge.\n",
    "load_params = {\n",
    "    'measurements': ['blue', 'red', 'nir', 'scl'],\n",
    "    'output_crs': native_crs,               # EPSG code\n",
    "    'resolution': (-20, 20),                # Target resolution\n",
    "    'group_by': 'solar_day',                # Scene ordering\n",
    "    'dask_chunks': {'x': 2048, 'y': 2048},  # Dask chunks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Apply the correct _Offset_ to the source data\n",
    "\n",
    "ESA introduced a change to their L1C processing that encodes their L1C and L2A products with an _Offset_ value such that\n",
    "`real_value = encoded_value * scale_factor + offset`\n",
    "\n",
    "Cloud data providers of ESA's S2A data additionally may have pre-applied the offset or not to the data that we index and load.\n",
    "\n",
    "This introduces an inconsistency in the S2A series that we need to account for.\n",
    "\n",
    "The scale and offset values, whether applied or not, are consistent.\n",
    "\n",
    "1. For each of the _datasets_ corresponding to a query, filter into lists of *offset-applied* and *offset-not-applied*\n",
    "1. Load each list into separate xarray Datasets\n",
    "1. Apply the _offset_ to the respective xarray Dataset\n",
    "1. Merge the two xarray Datasets\n",
    "1. Continue to apply masking and analytics as usual.\n",
    "\n",
    "These steps are undertaken in `load_s2l2a_with_offset_patch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = load_s2l2a_with_offset_patch(\n",
    "    dc, query | load_params\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Product definition\n",
    "\n",
    "Display the measurement definitions for the selected product.\n",
    "\n",
    "Use `list_measurements` to show the details for a product, and `masking.describe_variable_flags` to show the flag definitions.\n",
    "\n",
    "See [Appendix](#Appendix) for a Table of Sentinel-2 ARD bands and corresponding product measurement and alias names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurement definitions for the selected product\n",
    "Measurement_info = dc.list_measurements().loc[query['product']]\n",
    "Notebook_utils.heading(f'Measurement table for product: {query[\"product\"]}')\n",
    "display(measurement_info)\n",
    "\n",
    "# Separate lists of measurement names and flag names\n",
    "measurement_names = measurement_info[ pd.isnull(measurement_info.flags_definition)].index\n",
    "flag_names        = measurement_info[pd.notnull(measurement_info.flags_definition)].index\n",
    "\n",
    "notebook_utils.heading('Selected Measurement and Flag names')\n",
    "display(pd.DataFrame({\n",
    "    'group': ['Measurement names', 'Flag names'],\n",
    "    'names': [', '.join(measurement_names), ', '.join(flag_names)]\n",
    "}))\n",
    "\n",
    "# Flag definitions\n",
    "for flag in flag_names:\n",
    "    notebook_utils.heading(f'Flag definition table for flag name: {flag}')\n",
    "    display(masking.describe_variable_flags(data[flag]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flags_def = masking.describe_variable_flags(data[flag]).values\n",
    "flags_def = flags_def.tolist()[0][1]\n",
    "flags_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make SCL flags image\n",
    "flag_name = 'scl'\n",
    "flag_data = data[[flag_name]].where(valid_mask[flag_name]).persist()   # Dataset\n",
    "display(flag_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make SCL image\n",
    "# https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm\n",
    "# https://www.sentinel-hub.com/faq/how-get-s2a-scene-classification-sentinel-2/\n",
    "\n",
    "from bokeh.models.tickers import FixedTicker\n",
    "\n",
    "color_def = [\n",
    "    (0,  '#000000', 'No data'),   # black\n",
    "    (1,  '#ff0004', 'Saturated or defective'),   # red\n",
    "    (2,  '#868686', 'Dark features or shadows'),   # gray\n",
    "    (3,  '#774c0b', 'Cloud shadows'),   # brown\n",
    "    (4,  '#10d32d', 'Vegetation'),   # green\n",
    "    (5,  '#ffff53', 'Not vegetated'),   # yellow\n",
    "    (6,  '#0000ff', 'Water'),   # blue\n",
    "    (7,  '#818181', 'Unclassified'),   # medium gray\n",
    "    (8,  '#c0c0c0', 'Cloud medium probability'),   # light gray\n",
    "    (9,  '#f2f2f2', 'cloud high probability'),   # very light gray\n",
    "    (10, '#bbc5ec', 'Thin cirrus'),   # light blue/purple\n",
    "    (11, '#53fff9', 'Snow or ice'),   # cyan\n",
    "]\n",
    "color_val = [x[0] for x in color_def]\n",
    "color_hex = [x[1] for x in color_def]\n",
    "color_txt = [f'{x[0]:2d}: {x[2]}' for x in color_def]\n",
    "color_lim = (min(color_val), max(color_val) + 1)\n",
    "bin_edges = color_val + [max(color_val) + 1]\n",
    "bin_range = (color_val[0] + 0.5, color_val[-1] + 0.5)  # No idea why (0.5,11.5) works and (0,11) or (0,12) do not\n",
    "\n",
    "# These options manipulate the color map and colorbar to show the categories for this product\n",
    "options = {\n",
    "    'title': f'Flag data for: {query[\"product\"]} ({flag_name})',\n",
    "    'cmap': color_hex,\n",
    "    'clim': color_lim,\n",
    "    'color_levels': bin_edges,\n",
    "    'colorbar': True,\n",
    "    'width': 800,\n",
    "    'height': 450,\n",
    "    'aspect': 'equal',\n",
    "    'tools': ['hover'],\n",
    "    'colorbar_opts': {\n",
    "        'major_label_overrides': dict(zip(color_val, color_txt)),\n",
    "        'major_label_text_align': 'left',\n",
    "        'ticker': FixedTicker(ticks=color_val),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Set the Dataset CRS\n",
    "plot_crs = native_crs\n",
    "if plot_crs == 'epsg:4326':\n",
    "    plot_crs = ccrs.PlateCarree()\n",
    "\n",
    "\n",
    "# Native data and coastline overlay:\n",
    "# - Comment `crs`, `projection`, `coastline` to plot in native_crs coords\n",
    "# TODO: Update the axis labels to 'longitude', 'latitude' if `coastline` is used\n",
    "\n",
    "quality_plot = flag_data.hvplot.image(\n",
    "    x = 'x', y = 'y',                        # Dataset x,y dimension names\n",
    "    rasterize = True,                        # Use Datashader\n",
    "    aggregator = reductions.mode(),          # Datashader selects mode value, requires 'hv.Image'\n",
    "    precompute = True,                       # Datashader precomputes what it can\n",
    "    crs = plot_crs,                          # Datset crs\n",
    "    projection = ccrs.PlateCarree(),         # Output projection (ccrs.PlateCarree() when coastline=True)\n",
    "    coastline = '10m',                       # Coastline = '10m'/'50m'/'110m'\n",
    ").options(opts.Image(**options)).hist(bin_range = bin_range)\n",
    "\n",
    "# display(quality_plot)\n",
    "# Optional: Change the default time slider to a dropdown list, https://stackoverflow.com/a/54912917\n",
    "fig = pn.panel(quality_plot, widgets={'time': pn.widgets.Select})  # widget_location='top_left'\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mask layer\n",
    "\n",
    "good_pixel_flags = [flags_def[str(i)] for i in [4, 5, 6]]\n",
    "\n",
    "good_pixel_mask = enum_to_bool(data[flag_name], good_pixel_flags)  # -> DataArray\n",
    "display(good_pixel_mask)  # Type: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and nodata\n",
    "\n",
    "**TODO:** Update once the `load_s2l2a_with_offset_patch` is working, as this will have applied the scale and offset.\n",
    "\n",
    "The cell below then only applies the mask, which can likely be combined with the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a layer and apply masking and scaling, then persist in dask\n",
    "\n",
    "layer_name = 'nir'\n",
    "layer_list = ['blue', 'red', 'nir', 'scl']\n",
    "\n",
    "# Get scaling and offset values from product description\n",
    "scale = measurement_info.loc[layer_name].scale_factor\n",
    "offset = measurement_info.loc[layer_name].add_offset\n",
    "\n",
    "# Apply valid mask and good pixel mask (single layer)\n",
    "# layer = data[[layer_name]].where(valid_mask[layer_name] & good_pixel_mask) * scale + offset\n",
    "# layer = layer.persist()\n",
    "\n",
    "# Apply valid mask and good pixel mask (multiple layers)\n",
    "layer_set = data[layer_list].where(valid_mask[layer_list] & good_pixel_mask) * scale + offset\n",
    "layer_set = layer_set.persist()\n",
    "layer_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a plot\n",
    "\n",
    "options = {\n",
    "    'title': f'{query[\"product\"]}: {layer_name}',\n",
    "    'width': 800,\n",
    "    'height': 450,\n",
    "    'aspect': 'equal',\n",
    "    'cmap': cc.rainbow,\n",
    "    'clim': (0, 1),                          # Limit the color range depending on the layer_name\n",
    "    'colorbar': True,\n",
    "    'tools': ['hover'],\n",
    "}\n",
    "\n",
    "# Set the Dataset CRS\n",
    "plot_crs = native_crs\n",
    "if plot_crs == 'epsg:4326':\n",
    "    plot_crs = ccrs.PlateCarree()\n",
    "\n",
    "\n",
    "# Native data and coastline overlay:\n",
    "# - Comment `crs`, `projection`, `coastline` to plot in native_crs coords\n",
    "# TODO: Update the axis labels to 'longitude', 'latitude' if `coastline` is used\n",
    "\n",
    "layer_plot = layer.hvplot.image(\n",
    "    x = 'x', y = 'y',                        # Dataset x,y dimension names\n",
    "    rasterize = True,                        # Use Datashader\n",
    "    aggregator = reductions.mean(),          # Datashader selects mean value\n",
    "    precompute = True,                       # Datashader precomputes what it can\n",
    "    crs = plot_crs,                        # Dataset crs\n",
    "    projection = ccrs.PlateCarree(),         # Output projection (use ccrs.PlateCarree() when coastline=True)\n",
    "    coastline='10m',                         # Coastline = '10m'/'50m'/'110m'\n",
    ").options(opts.Image(**options)).hist(bin_range = options['clim'])\n",
    "\n",
    "# display(layer_plot)\n",
    "# Optional: Change the default time slider to a dropdown list, https://stackoverflow.com/a/54912917\n",
    "fig = pn.panel(layer_plot, widgets={'time': pn.widgets.Select})  # widget_location='top_left'\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Reproject\n",
    "\n",
    "Use xr_reproject to reproject to UTM data to a new project, e.g. to align with other datasets\n",
    "\n",
    "See [dea-notebooks/Frequently_used_code/Reprojecting_data.ipynb](#../../../dea-notebooks/Frequently_used_code/Reprojecting_data.ipynb) for additional examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example uses the query bounding box and epsg:4326  as the target\n",
    "\n",
    "target_crs = 'epsg:4326'\n",
    "target_res = (0.0002, 0.0002)   # approx. 20 m\n",
    "\n",
    "target = GeoBox.from_geopolygon(box(min_longitude, min_latitude, max_longitude, max_latitude, target_crs), target_res)\n",
    "layer_geo = xr_reproject(layer, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Dataset CRS\n",
    "plot_crs = target_crs\n",
    "if plot_crs == 'epsg:4326':\n",
    "    plot_crs = ccrs.PlateCarree()\n",
    "\n",
    "\n",
    "# Native data and coastline overlay:\n",
    "# - Comment `crs`, `projection`, `coastline` to plot in native_crs coords\n",
    "# TODO: Update the axis labels to 'longitude', 'latitude' if `coastline` is used\n",
    "    \n",
    "layer_plot = layer_geo.hvplot.image(\n",
    "    x = 'longitude', y = 'latitude',         # Dataset x,y dimension names\n",
    "    rasterize = True,                        # Use Datashader\n",
    "    aggregator = reductions.mean(),          # Datashader selects mean value\n",
    "    precompute = True,                       # Datashader precomputes what it can\n",
    "    crs = plot_crs,                          # Dataset crs\n",
    "    projection = ccrs.PlateCarree(),         # Output projection (use ccrs.PlateCarree() when coastline=True)\n",
    "    coastline='10m',                         # Coastline = '10m'/'50m'/'110m'\n",
    ").options(opts.Image(**options)).hist(bin_range = options['clim'])\n",
    "\n",
    "# display(layer_plot)\n",
    "# Optional: Change the default time slider to a dropdown list, https://stackoverflow.com/a/54912917\n",
    "fig = pn.panel(layer_plot, widgets={'time': pn.widgets.Select})  # widget_location='top_left'\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EC Copernicus Data Hub\n",
    "\n",
    "https://scihub.copernicus.eu\n",
    "\n",
    "EC Copernicus Data Hub is the primary source of S2 L1B and L2A (Sen2cor) data. L2A (Sen2cor) is available globally from December 2018 onwards.\n",
    "\n",
    "Atmospheric correction - Sen2cor\n",
    "- Detailed summary of bands and thresholds for classification: https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm\n",
    "- Details of Sen2cor paramaters: https://sentinels.copernicus.eu/documents/247904/685211/Sen2-Cor-L2A-Input-Output-Data-Definition-Document\n",
    "\n",
    "Atmospheric correction - Any other method, including Sen2cor prior to December 2018\n",
    "- EASI can inject just about any preprocessing routine into our data preparation pipeline. That is, we would download S2 L1B and apply a preprocessing routine (an atmospheric correction method). This would create a new product name like ```cophub_s2_[type]_[routine]```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentinel-2 Cloud-Optimized GeoTIFFs\n",
    "\n",
    "https://registry.opendata.aws/sentinel-2-l2a-cogs/\n",
    "\n",
    "Open and free S2 L2a (Sen2cor) COGs. Same time range as from CopHub (December 2018 onwards).\n",
    "\n",
    "TODO: Check the latency between CopHub and AWS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
