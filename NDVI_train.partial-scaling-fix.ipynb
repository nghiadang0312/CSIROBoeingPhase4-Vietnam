{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6b21f-1379-4f68-ba44-c979036e539d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# Common imports and settings\n",
    "import os, sys\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "from IPython.display import Markdown\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "import xarray as xr\n",
    "\n",
    "# Datacube\n",
    "import datacube\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils import masking\n",
    "from datacube.utils.cog import write_cog\n",
    "# https://github.com/GeoscienceAustralia/dea-notebooks/tree/develop/Tools\n",
    "from dea_tools.plotting import display_map, rgb\n",
    "from dea_tools.datahandling import mostcommon_crs\n",
    "\n",
    "# EASI defaults\n",
    "easinotebooksrepo = '/home/jovyan/easi-notebooks'\n",
    "if easinotebooksrepo not in sys.path: sys.path.append(easinotebooksrepo)\n",
    "from easi_tools import EasiDefaults, xarray_object_size, notebook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc7f71-3aaa-4f48-9b06-c990bac3c392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data tools\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Datacube\n",
    "from datacube.utils import masking  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/masking.py\n",
    "from odc.algo import enum_to_bool   # https://github.com/opendatacube/odc-algo/blob/main/odc/algo/_masking.py\n",
    "from odc.algo import xr_reproject   # https://github.com/opendatacube/odc-algo/blob/main/odc/algo/_warp.py\n",
    "from datacube.utils.geometry import GeoBox, box  # https://github.com/opendatacube/datacube-core/blob/develop/datacube/utils/geometry/_base.py\n",
    "\n",
    "# Holoviews, Datashader and Bokeh\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import panel as pn\n",
    "import colorcet as cc\n",
    "import cartopy.crs as ccrs\n",
    "from datashader import reductions\n",
    "from holoviews import opts\n",
    "# import geoviews as gv\n",
    "# from holoviews.operation.datashader import rasterize\n",
    "hv.extension('bokeh', logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e1dea-6328-42da-9e68-7e0716738608",
   "metadata": {},
   "source": [
    "## Dask cluster\n",
    "\n",
    "There are two dask cluster options available:\n",
    "\n",
    "- *LocalCluster* runs on the JupyterLab node so is limited to the amount of memory available (depending on the resource type selected at login). Dask does allow for using more than the available memory by swapping data to disk but there are practical limits to this. Good for testing and demonstrating smaller workflows.\n",
    "- *Dask Gateway* creates a *dask scheduler* and a set of *dask workers* that run in their own pods. Workers may still fail due to out-of-memory but if so the scheduler will attempt to replace the worker and reassign its tasks to other workers. Good for developing and running larger or quicker workflows.\n",
    "\n",
    "For more information see the set of notebooks at https://github.com/csiro-easi/easi-notebooks/tree/main/tutorials/dask.\n",
    "\n",
    "Here I have added the gateway option. You already had the LocalCluster code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f606084-c43f-41fd-95d1-6305047a22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask gateway\n",
    "\n",
    "cluster, client = notebook_utils.initialize_dask(use_gateway=True, workers=(1,10))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72901e1e-2af3-408c-816c-4c3412575de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "\n",
    "## Can also use the initialize_dask() helper function.\n",
    "## This function will attempt to re-use an existing LocalCluster if one is found.\n",
    "## cluster, client = notebook_utils.initialize_dask(use_gateway=False, workers=(1,4))\n",
    "\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# cluster = LocalCluster(n_workers=2, threads_per_worker=4)\n",
    "# # cluster.scale(n=2, memory=\"14GiB\")\n",
    "# cluster.scale(n=8, memory=\"6GiB\")\n",
    "# client = Client(cluster)\n",
    "# display(client)\n",
    "\n",
    "# dashboard_address = notebook_utils.localcluster_dashboard(client=client,server=easi.hub)\n",
    "# display(dashboard_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab034180-58ba-4efa-a77c-a21dcb7989dd",
   "metadata": {},
   "source": [
    "## Initialise Datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5cc87-77a7-4734-ae3e-13d7360624c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "easi = EasiDefaults()\n",
    "\n",
    "family = 'sentinel-2'\n",
    "product = easi.product(family)\n",
    "display(Markdown(f'Default {family} product for \"{easi.name}\": [{product}]({easi.explorer}/products/{product})'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c45e34-f8f3-4e38-b973-cd75a8dabc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube()\n",
    "\n",
    "# Access AWS \"requester-pays\" buckets\n",
    "# This is necessary for reading data from most third-party AWS S3 buckets such as for Landsat and Sentinel-2\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ec44e9-4fad-4213-9df6-4b2f3a214231",
   "metadata": {},
   "source": [
    "## Read and verify the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614ee7b-1f84-4f65-ba6b-09ecf790fd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_data_geo\n",
    "# import geopandas as gpd\n",
    "# from deafrica_tools.areaofinterest import define_area\n",
    "# from datacube.utils.geometry import Geometry\n",
    "# import xarray as xr\n",
    "train_path = \"train/Soc Trang_Traning.shp\"\n",
    "train = load_data_geo(train_path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e555881-6bed-4c0b-8b81-15ef4e7aef10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train1 = train.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f362c-ea1f-47be-92ee-7d5083fdaa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train1.head().explore(column=\"Name\", legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e868c9-6964-4533-96b5-144ecd158ba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Proposed workflow\n",
    "\n",
    "1. Get bounding polygon for all training data points\n",
    "1. Load datacube (dc.load with dask) for bounding polygon (and all times when you're ready to try that)\n",
    "    - Consider also remapping S2 data to lat/lon projection (e.g., epsg:4326) - may not be necessary\n",
    "1. Apply the S2 masking, scale, offset\n",
    "1. Calculate NDVI (still in dask so its a \"virtual\" on-demand calculation)\n",
    "1. Use xarray.persist() to pre-calculate NDVI for all pixels in our bounding polygon\n",
    "   - Its usually more efficient to read and process all pixels than process each training point\n",
    "1. For idx, point in train.iterrows():\n",
    "    -  Get points from xarray (dask)\n",
    "       - May need to convert (point lat/lon to S2 UTM) or (dc.load into epsg:4326)\n",
    "       - Xarray data is in S2 UTM project (output_crs, resolution)\n",
    "       - Point data is in epsg:4326 (train.crs)\n",
    "    -  Store the loaded point data in the dictionary with a key based on the point index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb46ad8-a7e2-4bcf-abba-401de8b18bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deafrica_tools.bandindices import calculate_indices\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Vietnam\n",
    "min_longitude, max_longitude = (105.5, 106.4)\n",
    "min_latitude, max_latitude = (9.2, 10.0)\n",
    "min_date = '2021-12-01' # 2021-11-01\n",
    "max_date = '2022-01-01' # 2022-01-01\n",
    "product = 's2_l2a'\n",
    "\n",
    "query1 = {\n",
    "    'product': product,                     # Product name\n",
    "    'x': (min_longitude, max_longitude),    # \"x\" axis bounds\n",
    "    'y': (min_latitude, max_latitude),      # \"y\" axis bounds\n",
    "    'time': (min_date, max_date),           # Any parsable date strings\n",
    "}\n",
    "\n",
    "# Most common CRS\n",
    "native_crs = notebook_utils.mostcommon_crs(dc, query1)\n",
    "\n",
    "query1.update({\n",
    "    'measurements': ['blue', 'green', 'red', 'nir', 'scl'],  # Selected measurement bands\n",
    "    'output_crs': native_crs,               # EPSG code\n",
    "    'resolution': (-10, 10),                # Target resolution\n",
    "    'group_by': 'solar_day',                # Scene ordering\n",
    "    'dask_chunks': {'x': 3310, 'y': 3000},  # Dask chunks\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f465a4-b86c-4f80-8894-135488bab39f",
   "metadata": {},
   "source": [
    "## Dask loading and chunking\n",
    "\n",
    "The main tuning options for _dask_ are:\n",
    "\n",
    "- The data _chunk_ size.\n",
    "- Using _.persist()_ to pre-calculate intermediate stages and _.compute()_ to finalise the result back to your Jupyter session.\n",
    "\n",
    "Here we can try different chunk shapes. A different chunk shape means that blocks of data get copied and calculated in different size aggregates, which can mean improved efficiency depending on the tasks being applied. We can change the chunk shape later if our algorithm will benefit from it.\n",
    "\n",
    "In the `xarray.Dataset` graphic below, select one of the \"stacked cylinder\" icons on the right of a Data variable row. This shows a graphic of the array. Edit the `dask_chunks` values in the `query1.update()` function above. The target parameters for good general use are:\n",
    "\n",
    "- Chunk size about 20 MB. It can be larger if need be but try not to it be too small.\n",
    "- Number of chunks in the \"Dask graph\". 100s is good, 1000s is fine, 10000s is probably getting too many. Its a balance or trade-off between chunk size and the number of chunks.\n",
    "- A low mod(dim-length, chunck-length) in each dimensionLimit small fractional chunks, thats is mod(dim-length, chunck-length) in each dimension.\n",
    "- Square chunks and binary multiple sizes are often used but are not as important as the other three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41388281-383c-43d6-b55d-5c283894b2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = 3310\n",
    "# y = 3000\n",
    "# print(f\"{9902.0/x} rem {9902 % x}\")\n",
    "# print(f\"{8874.0/y} rem {8874 % y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1332d-1b59-4102-a702-ef869939863a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = dc.load(**query1)\n",
    "\n",
    "notebook_utils.heading(notebook_utils.xarray_object_size(data))\n",
    "display(data)\n",
    "\n",
    "# Calculate valid (not nodata) masks for each layer\n",
    "valid_mask = masking.valid_data_mask(data)\n",
    "notebook_utils.heading('Valid data masks for each variable')\n",
    "display(valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bd626-c6a7-4c4b-8781-7840fe54abff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change variable name for dev\n",
    "data1 = dc.load(**query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0d2c1-c36b-4977-8c94-955e359f1b57",
   "metadata": {},
   "source": [
    "## Masking and scaling\n",
    "\n",
    "The scale factor and offset need to manually applied to the arrays (the ODC chooses not make an assumption about these).\n",
    "\n",
    "The Sentinel-2 L2A \"SCL\" band contains quality flags and information. These flags are distrete values (each pixel will have one of these values. In ODC we can use the `enum_to_bool()` function to create boolean mask from a combination of flag values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bfe944-edb5-4499-950e-9e2bc73c8373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the scale factor and offset from the measurement metadata\n",
    "\n",
    "measurement_info = dc.list_measurements().loc[query1['product']].loc[query1['measurements']]  # Pandas dataframe\n",
    "display(measurement_info)\n",
    "\n",
    "# The \"SCL\" band contains quality flags and information. The details can also be found in the metadata.\n",
    "\n",
    "flag_name = 'scl'\n",
    "flag_desc = masking.describe_variable_flags(data[flag_name])  # Pandas dataframe\n",
    "display(flag_desc)\n",
    "display(flag_desc.loc['qa'].values[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7fc8f-1de7-4e97-9777-b488864bab01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a \"data quality\" Mask layer\n",
    "\n",
    "flags_def = flag_desc.loc['qa'].values[1]\n",
    "good_pixel_flags = [flags_def[str(i)] for i in [4, 5]]  # To pass strings to enum_to_bool()\n",
    "\n",
    "# enum_to_bool calculates the pixel-wise \"or\" of each set of pixels given by good_pixel_flags\n",
    "# 1 = good data\n",
    "# 0 = \"bad\" data\n",
    "\n",
    "good_pixel_mask = enum_to_bool(data[flag_name], good_pixel_flags)  # -> DataArray\n",
    "# display(good_pixel_mask)  # Type: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc5f24-352b-407f-be91-4d8cd1b33b35",
   "metadata": {},
   "source": [
    "### Sentinel-2 scaling and offset changes\n",
    "\n",
    "ESA has undertaken a reprocessing of the Sentinel-2 L2A product, which includes a change to offset value used to convert digital numbers (in file) to scientific values (reflectances). ESA's reprocessing is flowing through to the [AWS open data repository of S2 L2A](https://registry.opendata.aws/sentinel-2/) but while this stabilises we may see inconsistencies in the COG files (encoded DNs) vs the STAC metadata (whether offset has been applied or not).\n",
    "\n",
    "**TL;DR**: DN values have different definitions with different processing baselines!\n",
    "\n",
    "[L2A algorithm and products](https://sentinels.copernicus.eu/web/sentinel/technical-guides/sentinel-2-msi/level-2a-algorithms-products): Starting with the PB 04.00 (25th January 2022), the dynamic range of the Level-2A products is shifted by a band-dependent constant: BOA_ADD_OFFSET. This offset will allow encoding negative surface reflectances that may occur over very dark surfaces\n",
    "\n",
    "```\n",
    "L2A_SRi = (L2A_DNi + BOA_ADD_OFFSETi) / QUANTIFICATION_VALUEi\n",
    "\n",
    "QUANTIFICATION_VALUEi = 10000\n",
    "BOA_ADD_OFFSETi = -1000\n",
    "\n",
    "refl = (dn -1000) / 10000\n",
    "refl = dn/10000 - 1000/10000\n",
    "refl = dn * 0.0001 - 0.1   # These are the values in the product definition, https://explorer.asia.easi-eo.solutions/products/s2_l2a.odc-product.yaml\n",
    "```\n",
    "\n",
    "**What can we do about?** We need to add a check for a `earthsearch:boa_offset_applied` property in each dataset's metadata, and adjust the `offset` value accordingly. [Example per scene workflow](https://github.com/Element84/earth-search/issues/23#issuecomment-1834674853)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24277b3e-278f-408c-8b80-4160e7d79c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply valid mask (calculated above) and good pixel mask with scale and offset for each data layer and merge the results\n",
    "# Optional - use .persist() on each layer or the result dataset\n",
    "\n",
    "scale = 0.0001\n",
    "offset = 0  # Assumes earthsearch:boa_offset_applied = True (else offset = -0.1)\n",
    "\n",
    "data_layer_names = [x for x in data.data_vars if x != 'scl']\n",
    "\n",
    "rs = []\n",
    "for layer_name in data_layer_names:\n",
    "    # Apply valid mask (calculated above) and good pixel mask with scale and offset\n",
    "    layer = data[[layer_name]].where(valid_mask[layer_name] & good_pixel_mask) * scale\n",
    "    rs.append(layer)\n",
    "    \n",
    "result = xr.merge(rs).persist()  # Calculate intermediate result\n",
    "result  # Type: float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7dc5f-bf11-42a4-b20a-9d65e6acf913",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate NDVI\n",
    "\n",
    "Two options:\n",
    "- `calculate_indices()` - a large set of Indices available. May change dataset variable names if satellite_mission is landsat or sentinel-2 (required).\n",
    "- band math - calculate manually, if `calculate_indices()` is not suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3d656-aeff-40fb-832b-feda881260a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Frequency sampling codes - https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases\n",
    "\n",
    "ds1 = calculate_indices(result, index='NDVI', satellite_mission='s2')\n",
    "ndvi = ds1[\"NDVI\"]\n",
    "average_ndvi = ndvi.resample(time='1M').mean().persist()  ## tính mean cho từng tháng -> time = 12\n",
    "average_ndvi  # DataArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6dc10-f695-40f9-b0dd-f98a4e5596da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A standard matplotlib picture\n",
    "# Will struggle for large datasets and may kill the kernel (out of memory)\n",
    "# Creating an image will trigger processing of the dask tasks. Open the dask dashboard to see progress.\n",
    "\n",
    "average_ndvi.plot(col=\"time\", size=5, clim=(0,1), cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0605943-1cd8-4009-a6fd-cacee60dcee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# An interactive holoviz plot\n",
    "# Works for large datasets with dask\n",
    "# An image of the data is created on-demand (see https://datashader.org/getting_started/Pipeline.html)\n",
    "\n",
    "xx = ndvi.to_dataset().hvplot(\n",
    "    groupby='time',rasterize=True,\n",
    "    cmap=\"RdYlGn\", clim=(0,1),\n",
    "    height=500,\n",
    ")\n",
    "xx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b034485-8127-4bee-bdb0-06de915e2612",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract training points from the average NDVI dataset\n",
    "\n",
    "*Note:* This section has not been fully check/tested (by Matt) yet. May need to filter `NaN` values from `load_datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42990949-d84e-49fd-9288-ebb6b850bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_datasets = {}\n",
    "for idx, point in train.iterrows():\n",
    "    key = f\"point_{idx + 1}\"\n",
    "    try:\n",
    "        loaded_datasets[key] = {\n",
    "            \"NDVI\": average_ndvi.sel(x=point.geometry.x, y=point.geometry.y, method='nearest').values,\n",
    "            \"label\": point.Name\n",
    "                               }\n",
    "    except Exception as e:\n",
    "        # loaded_datasets[key] = None\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449ea08-1e40-41ae-9fc3-b513a1347cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e8a98e-fe41-4e85-82b7-91d41cae712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tiền xử lý data: fill nan, remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b490d-351e-407a-b615-8ef3a5f07b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the labels\n",
    "labels = train.Name.values\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "label_mapping = dict(zip(labels, numeric_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73600e9-b8bf-4bad-887e-35c3cad269c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27438db4-9418-4e56-a480-028c9e22901f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for k, v in loaded_datasets.items():\n",
    "    X.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf519d4-9683-40e6-ae5c-21698a2f42bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fd15a-8892-4b98-9d83-f8ce1d0fcbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_new = []\n",
    "lb_new = []\n",
    "for i in range(len(X)):\n",
    "    if X[i] is not None:\n",
    "        x_new.append(X[i])\n",
    "        lb_new.append(numeric_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ce7869-5d28-4a0f-adf2-e24c68e02360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71fb36-25ba-4ffa-95f9-64dc662574dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_means = np.nanmean(x_new, axis=0)\n",
    "column_means_expanded = np.tile(column_means, (len(x_new), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812617c-47bc-4fa6-8f1f-8cac9c1f6b23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_final = np.where(np.isnan(x_new), column_means_expanded, x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640853-408b-4b91-b179-3add07710891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c5fe0-013e-46eb-89dc-271571d70a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_final, lb_new, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e88651-2706-4cd8-a151-b8586c8f15c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, random_state=42, criterion='gini', max_depth=10)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384e848-023d-4bd5-85f7-7c183d50f548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278a598-5420-4faf-af6b-31a075e88b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tạo RandomForestClassifier mặc định để sử dụng làm mô hình ban đầu trong pipeline\n",
    "base_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Tạo pipeline\n",
    "pipeline = Pipeline([\n",
    "    # ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', base_model),\n",
    "])\n",
    "\n",
    "# Thiết lập các tham số bạn muốn tối ưu hóa\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 500, 1000],\n",
    "    'classifier__max_depth': [5, 10, 20],\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "# Sử dụng GridSearchCV để tìm bộ tham số tốt nhất\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# In ra bộ tham số tốt nhất\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Dự đoán trên tập kiểm tra\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Đánh giá kết quả\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc029382-5431-44f1-a5e2-a4f564477f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg = average_ndvi.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618067a-e35c-4410-baa1-30994aa9021a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg1 = avg.fillna(avg.mean(dim='x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb0f74-be7b-4d27-8bd9-b38612ec3be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_search.predict([avg1.isel(y=0, x=0).values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d451d68-a208-4059-98b8-b5b114be15ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be97b7-c71f-4bd5-be0f-9cad0f29d190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
